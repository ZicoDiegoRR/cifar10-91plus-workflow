{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "EeoM51gwM_Oi",
        "Hb1R18axNRn8",
        "Du6nTc0rOPFb",
        "X4Tr5jSuPl0P",
        "hVWEFNjKQXyU",
        "Rbn2jFdvSaxG",
        "4R1AhQtvU1-s",
        "OvpeL7RsY_rl",
        "zk9TbrZPdA1a"
      ],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNoyhlVZWliEvsLWISGpy6H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZicoDiegoRR/cifar10-93plus-workflow/blob/main/cifar_10_workflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CIFAR-10 Custom CNN Architecture with 91+% Test Accuracy**"
      ],
      "metadata": {
        "id": "VTibMEuyGRJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we'll explore the power of CNN in classifying images based on 10 defined labels, such as:\n",
        "\n",
        "1.   Airplane\n",
        "2.   Automobile\n",
        "3.   Bird\n",
        "4.   Car\n",
        "5.   Deer\n",
        "6.   Dog\n",
        "7.   Frog\n",
        "8.   Horse\n",
        "9.   Ship\n",
        "10.  Truck\n",
        "\n",
        "<br>\n",
        "\n",
        "We'll train a model using our own custom neural network architecture with `nn.Conv2d`, `nn.BatchNorm2d`, `nn.SiLU`, `nn.AdaptiveAvgPool2d`, `nn.Linear`, and `nn.Dropout` modules.\n",
        "\n",
        "Before we continue, it would be helpful if you read this quick summary about those modules in action so you won't be clueless about the architecture. Let's break them down!\n",
        "\n",
        "- `nn.Conv2d` processes tensor images by applying a 2D convolution over a quantized 2D input. In short, this is one of the most important network to have when working with CNN.\n",
        "- `nn.BatchNorm2d` normalizes and stabilizes the training by applying batch normalization. There's a dedicated formula for this, but this isn't a research paper.\n",
        "- `nn.SiLU` applies Sigmoid Linear Unit to the gradients. This is especially helpful for non-linear patterns and more powerful than `nn.ReLU()` in most cases.\n",
        "- `nn.AdaptiveAvgPool2d` applies a 2D average pooling over an input signal. This works similarly to `nn.MaxPool2d`, but this finds the average value instead of the biggest value.\n",
        "- `nn.Linear` applies a linear transformation to the input.\n",
        "- `nn.Dropout` randomly zeroes the weight to force the model to not strictly rely on defined networks, enhancing generalization.\n",
        "\n",
        "Now that we've covered all of them, let's go train our model, shall we?\n",
        "(GPU acceleration is recommended to continue)"
      ],
      "metadata": {
        "id": "BqHjc_9gGe69"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Import the mandatory libraries and modules"
      ],
      "metadata": {
        "id": "EeoM51gwM_Oi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Built-in module\n",
        "import os\n",
        "\n",
        "# The powerhouse libraries\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "# To-shorten-the-function modules\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# For visualization\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "HIltkn80EKXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Let's define the path and the device for our model"
      ],
      "metadata": {
        "id": "Hb1R18axNRn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is where we'll save our model\n",
        "weight_save_path = \"/content/training/CIFAR-10\"\n",
        "os.makedirs(weight_save_path, exist_ok=True)\n",
        "\n",
        "# GPU or CPU?\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\""
      ],
      "metadata": {
        "id": "mHwdJvGpE1kD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Now we can download the dataset and compose the transformation method"
      ],
      "metadata": {
        "id": "Du6nTc0rOPFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the mean and the standard deviation, very useful to normalize the data\n",
        "def get_mean_std():\n",
        "    # Still a simple transformation method\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    # Now we download and transform the images into tensors\n",
        "    cifar10_dataset = torchvision.datasets.CIFAR10(\n",
        "        root=\"/content/training\",\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=transform\n",
        "    )\n",
        "\n",
        "    # After downloading, we can load them\n",
        "    cifar10_load = DataLoader(\n",
        "        cifar10_dataset,\n",
        "        shuffle=False,\n",
        "        num_workers=1,\n",
        "        batch_size=50000,\n",
        "    )\n",
        "\n",
        "    # Let's obtain the image data\n",
        "    data = next(iter(cifar10_load))[0]\n",
        "\n",
        "    # Mean and standard deviation can be calculated here\n",
        "    mean = torch.mean(data, dim=(0, 2, 3))\n",
        "    std = torch.std(data, dim=(0, 2, 3))\n",
        "\n",
        "    # Return the values\n",
        "    return mean, std\n",
        "\n",
        "# We get the mean and the standard deviation values\n",
        "mean, std = get_mean_std()\n",
        "\n",
        "# Now we create another transformation method with normalization\n",
        "transform_flow = [\n",
        "    transforms.ToTensor(), # Convert the data into tensors\n",
        "    transforms.Normalize((mean), (std)) # Normalize the data\n",
        "]\n",
        "\n",
        "# Let's add separate transformation methods with data augmentation for the training data (better for generalization)\n",
        "train_transform, test_transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
        "        # The code ColorJitter applies random color jitter, like brightness, contrast, saturation, and hue commonly found in image editor.\n",
        "        transforms.RandomCrop(32, padding=4), # Crop the images randomly\n",
        "        transforms.RandomHorizontalFlip(), # Flip them\n",
        "    ] + transform_flow\n",
        "), transforms.Compose(transform_flow)"
      ],
      "metadata": {
        "id": "zZvFPSSeElBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Let's prepare the data for training and testing"
      ],
      "metadata": {
        "id": "X4Tr5jSuPl0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the training data\n",
        "train_dataset = torchvision.datasets.CIFAR10(\n",
        "    root=\"/content/training\", # The path to put the images\n",
        "    train=True, # This argument is needed to determine if the data is for training or not (helpful for predefined datasets)\n",
        "    download=True, # Enable downloading\n",
        "    transform=train_transform # Applying transformation with data augmentation\n",
        ")\n",
        "\n",
        "# Same drill like above, but for validation\n",
        "test_dataset = torchvision.datasets.CIFAR10(\n",
        "    root=\"/content/training\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=test_transform # Applying transformation without data augmentation\n",
        ")"
      ],
      "metadata": {
        "id": "OVfW5vMvEwQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: It's time to load the data"
      ],
      "metadata": {
        "id": "hVWEFNjKQXyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the data\n",
        "train_tensor = DataLoader(\n",
        "    train_dataset, # The data for training\n",
        "    shuffle=True, # Optional, but shuffling sometimes increases the accuracy\n",
        "    num_workers=1, # Counting how many subprocesses to load the data\n",
        "    batch_size=128 # How many images per batch (imagine slicing a pizza in different size)\n",
        ")\n",
        "\n",
        "# Same drill as above, but for validation\n",
        "test_tensor = DataLoader(\n",
        "    test_dataset, # The data for validating\n",
        "    shuffle=False,\n",
        "    num_workers=1,\n",
        "    batch_size=128\n",
        ")"
      ],
      "metadata": {
        "id": "QKf7mdkoE8fQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: The most confusing part â€” let's define our neural network"
      ],
      "metadata": {
        "id": "Rbn2jFdvSaxG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, you can experiment with the layers and/or the values. But, keep in mind that modifying them usually leads to instability, inefficent parameters (too many parameters for minimal gain), or longer training time."
      ],
      "metadata": {
        "id": "SsZWpAeMSwNT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# This is the neural network for residual network\n",
        "class ResNetBlock(nn.Module):\n",
        "    def __init__(self, input, out, reduce=False): # `reduce` triggers downsampling process\n",
        "        super(ResNetBlock, self).__init__()\n",
        "\n",
        "        # This is the first convolution stack\n",
        "        self.first_layer = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                input,\n",
        "                out,\n",
        "                kernel_size=3,\n",
        "                padding=1,\n",
        "                bias=False,\n",
        "                stride=1 if not reduce else 2\n",
        "                # Stride value 2 downsamples the input for less computation\n",
        "            ),\n",
        "            nn.BatchNorm2d(out),\n",
        "            nn.SiLU(),\n",
        "        )\n",
        "\n",
        "        # This is the second convolution stack\n",
        "        self.second_layer = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                out,\n",
        "                out,\n",
        "                kernel_size=3,\n",
        "                padding=1,\n",
        "                bias=False,\n",
        "                stride=1\n",
        "            ),\n",
        "            nn.BatchNorm2d(out),\n",
        "            nn.SiLU(),\n",
        "        )\n",
        "\n",
        "        # This is the skip connection\n",
        "        self.skip = nn.Conv2d(\n",
        "            input, out, kernel_size=1, bias=False, stride=1 if not reduce else 2,\n",
        "        ) if input != out else nn.Identity()\n",
        "\n",
        "    # The forward pass\n",
        "    def forward(self, x):\n",
        "        # Computing the input through a convolution layer\n",
        "        res = self.skip(x)\n",
        "\n",
        "        # Feeding the input to the hidden layers\n",
        "        out = self.second_layer(self.first_layer(x))\n",
        "\n",
        "        # Adding the input to the output (skip connection)\n",
        "        return out + res\n",
        "\n",
        "# This is the main neural network\n",
        "class CIFAR_10(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CIFAR_10, self).__init__()\n",
        "\n",
        "        # This is a convolution stack\n",
        "        self.conv_layer = nn.Sequential(\n",
        "            ResNetBlock(3, 32),\n",
        "            ResNetBlock(32, 64),\n",
        "            ResNetBlock(64, 128, reduce=True),\n",
        "            ResNetBlock(128, 256, reduce=True),\n",
        "            ResNetBlock(256, 512),\n",
        "            ResNetBlock(512, 1024),\n",
        "        )\n",
        "\n",
        "        # This is a linear stack with AdaptiveAvgPool2d and Flatten\n",
        "        self.classify_layer = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Flatten(),\n",
        "            self.lin_layer(1024, 512, dropout=0.2),\n",
        "            self.lin_layer(512, 256),\n",
        "            self.lin_layer(256, 128, dropout=0.2),\n",
        "            self.lin_layer(128, 64),\n",
        "            nn.Linear(64, 10)\n",
        "        )\n",
        "\n",
        "        # We use two skip connections in this example\n",
        "        self.skips = nn.ModuleList([\n",
        "            self.resnet(3, 128, stride=2),\n",
        "            self.resnet(3, 1024, stride=4)\n",
        "        ])\n",
        "\n",
        "\n",
        "    # For encapsulation\n",
        "    def lin_layer(self, input, out, dropout=0.0):\n",
        "        dropout = nn.Dropout(dropout) if dropout != 0.0 else nn.Identity()\n",
        "        return nn.Sequential(\n",
        "            nn.Linear(input, out), dropout,\n",
        "        )\n",
        "\n",
        "    # For encapsulation\n",
        "    def resnet(self, in_channels, out_channels, stride):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.SiLU(),\n",
        "        )\n",
        "\n",
        "    # The forward pass\n",
        "    def forward(self, x):\n",
        "        # Here, we add the first skip connection to the first three convolution stack\n",
        "        first_down = self.conv_layer[:3](x) + self.skips[0](x)\n",
        "\n",
        "        # Same as above, but we use the second skip connection and the last three convolution stack\n",
        "        second_down = self.conv_layer[3:](first_down) + self.skips[1](x)\n",
        "\n",
        "        # We classify the weight and return the prediction\n",
        "        return self.classify_layer(second_down)\n",
        "\n",
        "# This is our model loaded into the available device\n",
        "cifar10_model = CIFAR_10().to(device)"
      ],
      "metadata": {
        "id": "q0jWm2T8FFfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: For performance and quality, let's tune our hyperparameters"
      ],
      "metadata": {
        "id": "4R1AhQtvU1-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You can change the hyperparameters, but beware of instability!\n",
        "# SGD optimizes the training by optimizing the gradients. This isn't an adaptive learning, so the model\n",
        "# will converge slowly because of fixed learning rate\n",
        "optimizer = optim.SGD(\n",
        "    cifar10_model.parameters(),\n",
        "    lr=0.01, # Custom fixed learning rate\n",
        "    weight_decay=10**(-6), # Penalizing weights to encourage generalization\n",
        "    momentum=0.9 # Smoothes the gradient updates\n",
        ")\n",
        "\n",
        "epochs = 100 # Defines how many times to repeat the training\n",
        "loss_hist = [] # Saves the losses and the accuracy for visualization\n",
        "\n",
        "# Learning rate scheduler optimizes and modifies the learning rate to make updates optimal\n",
        "lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
        "    optimizer, # The optimizer\n",
        "    T_max=epochs, # This tells the scheduler the amount of steps so that\n",
        "                  # the learning rate scheduler can decay the learning rate gradually\n",
        ")\n",
        "\n",
        "# This is the loss function using CrossEntropyLoss for classification task\n",
        "# Using label smoothing smoothes the loss calculation for precise accuracy\n",
        "loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "# This section covers the early stopping variables\n",
        "wait = 0 # This is the wait time before triggering the halt\n",
        "tolerance = 10 # Determines how many iterations needed before halting the training\n",
        "best_test_loss = np.inf # Saves the best test loss during training\n",
        "early_stopping = False # Toggleable, this stops the training when the test loss stops improving to save you some time\n",
        "hopeful_early_stopping = True # Toggleable, but needs early_stopping variable set to True to work.\n",
        "# 'hopeful_early_stopping' lets the early stopping mechanism to only allow consecutive increase in test loss."
      ],
      "metadata": {
        "id": "Q2GD3mNSFTpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final step: What are we waiting for? Let's train it!"
      ],
      "metadata": {
        "id": "OvpeL7RsY_rl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ei_ifbQqDxya"
      },
      "outputs": [],
      "source": [
        "for epoch in range(epochs): # To repeat the training\n",
        "    print(f\"\\nEpoch: {epoch+1}\")\n",
        "    cifar10_model.train() # This enables 'training' mode for the model\n",
        "\n",
        "    running_train_loss = 0.0 # This is to calculate the training loss. Initial value is zero.\n",
        "    for images, labels in train_tensor: # We get the image tensors and the labels\n",
        "        optimizer.zero_grad() # We reset the gradient for fresh epochs\n",
        "        output = cifar10_model(images.to(device)) # Feeding the data into the model\n",
        "\n",
        "        loss = loss_fn(output, labels.to(device)) # Calculating the training loss\n",
        "        loss.backward() # Backpropagation to calculate the gradient again\n",
        "        optimizer.step() # The optimizer will optimize the learning\n",
        "        running_train_loss += loss.item() * images.size(0) # We add the loss to the previous value because we work with multiple batches\n",
        "\n",
        "    epoch_train_loss = running_train_loss / len(train_tensor.dataset) # Calculating the average loss per batch\n",
        "\n",
        "    cifar10_model.eval() # Turning on the `evaluation` mode for the model\n",
        "    total = 0 # This is to count how many images are there\n",
        "    correct = 0 # And, this is to count how many times the model classify the images correctly\n",
        "    running_test_loss = 0.0 # This is to count the test loss. Zero is the initial value\n",
        "    with torch.no_grad(): # Disabling gradient calculation since we're not training the model at this phase\n",
        "        for images, labels in test_tensor: # We get the image tensors and the labels\n",
        "            images, labels = images.to(device), labels.to(device) # Moving the tensors and the labels to the correct device to avoid error\n",
        "            test_output = cifar10_model(images) # Now, we classify again with unseen data in the training\n",
        "\n",
        "            test_loss = loss_fn(test_output, labels) # We count the test loss\n",
        "            running_test_loss += test_loss.item() * images.size(0) # And, we add the loss value to the previous one because we work in batch\n",
        "\n",
        "            _, pred = torch.max(test_output.data, 1) # Classifying again with previous output, but adding more confidence to the model\n",
        "            total += labels.size(0) # We add the amount of images to the total variable\n",
        "            correct += (pred == labels).sum().item() # Now, we add the amount of correct labels from the model\n",
        "\n",
        "    epoch_test_loss = running_test_loss / len(test_tensor.dataset) # We count the average test loss per batch\n",
        "    acc = 100 * correct/total # We get the accuracy from here\n",
        "    lr_scheduler.step(epoch_test_loss) # The learning rate scheduler adjusts the learning rate based on the last test loss\n",
        "\n",
        "    # This part is for visualization\n",
        "    loss_hist.append([\n",
        "        epoch_train_loss,\n",
        "        epoch_test_loss,\n",
        "        acc/100,\n",
        "    ])\n",
        "\n",
        "    print(f\"Training Loss: {epoch_train_loss:.4f}\\nTest Loss: {epoch_test_loss:.4f}\\nAccuracy: {acc:.2f}%\")\n",
        "\n",
        "    # Early stopping\n",
        "    if early_stopping: # Checking if early stopping is True\n",
        "        if epoch_test_loss < best_test_loss: # This will check if the current test loss is better than the last best test loss\n",
        "            best_test_loss = epoch_test_loss # We change the variable of the besy test loss\n",
        "\n",
        "            # If the hopeful early stopping is True, it'll reset the wait time when the test loss decreases\n",
        "            if hopeful_early_stopping and wait > 0:\n",
        "                wait = 0\n",
        "                print(\"Detected a decrease in test loss. Resetting the wait...\")\n",
        "            torch.save(cifar10_model, os.path.join(weight_save_path, f\"Epoch_{epoch+1}.pth\")) # Saving the model\n",
        "\n",
        "        # Adds one to the wait value when an increase in test loss is detected\n",
        "        else:\n",
        "            wait += 1\n",
        "            print(f\"Detected an increase in test loss. ({wait}/{tolerance})\")\n",
        "\n",
        "        # This triggers the early stopping when the wait time is equal to the tolerance or patience\n",
        "        if wait == tolerance:\n",
        "            print(f\"Overfitting detected. ({wait}/{tolerance})\\nTriggering early stopping...\")\n",
        "            break\n",
        "\n",
        "    # If not early stopping, then save every state of the model\n",
        "    else:\n",
        "        torch.save(cifar10_model, os.path.join(weight_save_path, f\"Epoch_{epoch+1}.pth\"))\n",
        "\n",
        "print(\"Training finished.\")\n",
        "loss_hist.append(len(loss_hist)) # Adds the epoch count to the history for visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional: Visualize the loss and the acccuracy trends"
      ],
      "metadata": {
        "id": "zk9TbrZPdA1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplot_mosaic([[1]], figsize=(10, 5), layout=\"constrained\")\n",
        "loss_list_hist = [element for element in loss_hist if isinstance(element, list)]\n",
        "x_axis = np.arange(len(loss_list_hist)) + 1\n",
        "labels = [\"Training Loss\", \"Test Loss\", \"Accuracy\"]\n",
        "best_epoch_test_loss = []\n",
        "best_acc = []\n",
        "for i in range(3):\n",
        "    y_values = [element[i] for element in loss_list_hist]\n",
        "    ax[1].plot(x_axis, y_values, label=labels[i])\n",
        "    if i == 1:\n",
        "        best_epoch_test_loss = ([np.argmin(y_values) + 1, np.min(y_values)])\n",
        "    elif i == 2:\n",
        "        best_acc = ([np.argmax(y_values) + 1, np.max(y_values)])\n",
        "ax[1].set_xlabel(\"Epoch\")\n",
        "ax[1].set_title(\"CIFAR-10 Model's Accuracy and Loss per Epoch\")\n",
        "ax[1].grid(True)\n",
        "ax[1].legend()\n",
        "plt.savefig(\"/content/cifar10.png\")\n",
        "plt.show()\n",
        "print(f\"Epoch with the lowest test loss is the {best_epoch_test_loss[0]} epoch with {best_epoch_test_loss[1]:.4f} loss.\")\n",
        "print(f\"Epoch with the highest accuracy is the {best_acc[0]} epoch the with {best_acc[1]*100:.2f}% accuracy.\")"
      ],
      "metadata": {
        "id": "gm1f2NmBdPW4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}